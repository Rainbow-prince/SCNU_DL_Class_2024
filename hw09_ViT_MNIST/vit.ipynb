{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/pdf/2010.11929.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pip install einops and pytorch-gpu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import einops\n",
    "import copy\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.animation import FuncAnimation\n",
    "%matplotlib inline\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read ViT paper and anwser the following TODO questions\n",
    "### https://arxiv.org/pdf/2010.11929"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data \n",
    "#### turn on download=True if you use Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('./data', transform=torchvision.transforms.ToTensor(), download=False, train=True),\n",
    "    batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('./data', transform=torchvision.transforms.ToTensor(), download=False, train=False),\n",
    "    batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def accuracy(model, label='test', silent=False):\n",
    "    model.eval()\n",
    "    dataset = data_test if label == 'test' else data\n",
    "    count = 0\n",
    "    correct = 0\n",
    "    for x, y in dataset:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        res = model(x).argmax(-1)\n",
    "        count += len(x)\n",
    "        correct += (res == y).float().sum()\n",
    "    acc = correct / count\n",
    "    if not silent: print(f'accuracy on {label:5}: {acc}')\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(*args, n_col=None, figsize=None, block_size=3, labels=None):\n",
    "    '''Plot images in a grid\n",
    "    usage: plot(img1, img2, ...)\n",
    "    optional: n_col=3, figsize=(12, 8), block_size=1\n",
    "\n",
    "    img: torch.Tensor of shape (H, W) or (C, H, W) or [(H, W), ...] or [(C, H, W), ...]\n",
    "    '''\n",
    "    imgs = []\n",
    "    for img in args:\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            if img.dim() == 4: imgs.extend([img[i] for i in range(img.shape[0])])\n",
    "            else: imgs.append(img)\n",
    "        if isinstance(img, list) or isinstance(img, tuple):\n",
    "            assert all(isinstance(i, torch.Tensor) and i.dim() == 3 for i in img)\n",
    "            imgs.extend(img)\n",
    "\n",
    "    # flatten channels\n",
    "    imgs = [img.squeeze(0) if img.dim() == 3 else img for img in imgs]\n",
    "    imgs = [img.detach().cpu() for img in imgs]\n",
    "\n",
    "    if n_col is None: n_col = len(imgs)\n",
    "    n_row = math.ceil(len(imgs) / n_col)\n",
    "    if figsize is None: figsize = (n_col * block_size, n_row * block_size)\n",
    "    # normalize ax to 2d array\n",
    "    fig, ax = plt.subplots(n_row, n_col, figsize=figsize)\n",
    "    if n_row == 1 and n_col == 1: ax = np.array([[ax]])\n",
    "    elif n_row == 1: ax = ax.reshape((1, -1))\n",
    "    elif n_col == 1: ax = ax.reshape((-1, 1))\n",
    "    # plot\n",
    "    for i, img in enumerate(imgs):\n",
    "        idx = (i // n_col, i % n_col)\n",
    "        ax[idx].imshow(img)\n",
    "        if labels and i < len(labels): ax[idx].set_title(labels[i])\n",
    "    # remove the axis\n",
    "    for i in range(n_row * n_col):\n",
    "        idx = (i // n_col, i % n_col)\n",
    "        ax[idx].set_xticks([])\n",
    "        ax[idx].set_yticks([])\n",
    "        if i >= len(imgs): ax[idx].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test plot function\n",
    "def test_plot(scale=3):\n",
    "    imgs = next(iter(data))[0]\n",
    "    plot(imgs[0], block_size=scale)\n",
    "    plot(imgs[1], imgs[2], block_size=scale)\n",
    "    plot(imgs[3], imgs[4], n_col=scale, block_size=scale)\n",
    "    plot(imgs[5], imgs[6], imgs[7], n_col=2, block_size=scale)\n",
    "    plot(imgs[:5], n_col=3, block_size=scale)\n",
    "\n",
    "# test_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chunk images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "images = next(iter(data))[0][:2]\n",
    "plot(images[0])\n",
    "print(images.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the 4x4 image chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "b, c, h, w = images.shape\n",
    "# cut into tiles\n",
    "tile_size = 7\n",
    "plot(images[0])\n",
    "print(images[0].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use einops to speed the chunking'\n",
    "### Explain TODO in comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 1, 7, 7])\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1333]])\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1333]])\n"
     ]
    }
   ],
   "source": [
    "# cut into tiles\n",
    "tile_size = 7\n",
    "tiles = einops.rearrange(images, 'b c (h t1) (w t2) -> b (h w) c t1 t2', t1=tile_size, t2=tile_size)\n",
    "plot(tiles[0], n_col=4, block_size=1)\n",
    "print(tiles.size())\n",
    "# the tiles have size (Batch, 16=4x4, channel=1, H=7, W=7), that is the tokens are sequential\n",
    "# explain why tiles[0, 5, ...] are equal to images[B=0,C=0,H=7:14,W=7:14]\n",
    "# hint: the tiles[0, 5] is 5-th token of first image in batch, which is the row=1,col=1 token of size 7x7\n",
    "# the image pixels are ?\n",
    "print(tiles[0,5,0,:])\n",
    "print(images[0,0,7:14,7:14])\n",
    "assert torch.allclose(tiles[0,5,0,:],images[0,0,7:14,7:14])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder parameters\n",
    "### TODO: explain what is the value of context_size and its meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_HEADS = 2\n",
    "N_BLOCKS = 2\n",
    "VOCAB_SIZE = 10\n",
    "EMBED_SIZE = 16\n",
    "HIDDEN_SIZE = 32\n",
    "IMG_SIZE = 28\n",
    "CHANNEL = 1\n",
    "TILE_SIZE = 7\n",
    "assert IMG_SIZE % TILE_SIZE == 0\n",
    "CONTEXT_SIZE = 1 + (IMG_SIZE // TILE_SIZE) ** 2\n",
    "LEARNING_RATE = 3e-4\n",
    "DROPOUT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONTEXT_SIZE  # why add 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sinuoid positional embedding\n",
    "### TODO: refer to Transformer paper and explain the function of the position encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sinusoidal_positional_encoding(context_size=CONTEXT_SIZE, embed_size=EMBED_SIZE):\n",
    "  position = torch.arange(context_size).unsqueeze(1)\n",
    "  div_term = torch.exp(torch.arange(0, embed_size, 2) * -(math.log(10000.0) / embed_size))\n",
    "  positional_encoding = torch.zeros(context_size, embed_size)\n",
    "  positional_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "  positional_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "  return positional_encoding.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Explain each step in MultiheadAttention forward function and associate with the QKV equation\n",
    "### TODO: Explain why self.qkv is one linear function instead of three\n",
    "### TODO: Explain why q,k,v transpose(1,2) and y transpose back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "  def __init__(self, n_heads, embed_size, dropout, n_block=0):\n",
    "    super().__init__()\n",
    "    assert embed_size % n_heads == 0, f'{embed_size=} should be divisible by {n_heads=}'\n",
    "    self.n_heads = n_heads\n",
    "    self.embed_size = embed_size\n",
    "    self.dropout_rate = dropout\n",
    "    self.n_block = n_block  # The No. of this MHA\n",
    "    self.qkv = nn.Linear(embed_size, embed_size * 3, bias=False) # generate Q, K, V all at once\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    self.ln = nn.LayerNorm(embed_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    B, C, E = x.shape\n",
    "    # pre-layernorm\n",
    "    x = self.ln(x)\n",
    "    q, k, v = self.qkv(x).chunk(3, dim=-1)\n",
    "    # split heads\n",
    "    q = q.view(B, C, self.n_heads, self.embed_size // self.n_heads).transpose(1, 2)\n",
    "    k = k.view(B, C, self.n_heads, self.embed_size // self.n_heads).transpose(1, 2)\n",
    "    v = v.view(B, C, self.n_heads, self.embed_size // self.n_heads).transpose(1, 2)\n",
    "    # compute QKV\n",
    "    correlation = q @ k.transpose(-2, -1)\n",
    "    correlation = correlation / math.sqrt(k.shape[-1])\n",
    "    correlation = F.softmax(correlation, dim=-1)\n",
    "    correlation = F.dropout(correlation, p=self.dropout_rate, training=self.training)\n",
    "    y = correlation @ v\n",
    "    # combine heads\n",
    "    y = y.transpose(1, 2).contiguous().view(B, C, self.embed_size)\n",
    "    y = self.dropout(y)\n",
    "    return y\n",
    "\n",
    "  def forward_visualize(self, x):\n",
    "    B, C, E = x.shape\n",
    "    # pre-layernorm\n",
    "    x = self.ln(x)\n",
    "    q, k, v = self.qkv(x).chunk(3, dim=-1)\n",
    "    # split heads\n",
    "    q = q.view(B, C, self.n_heads, self.embed_size // self.n_heads).transpose(1, 2)\n",
    "    k = k.view(B, C, self.n_heads, self.embed_size // self.n_heads).transpose(1, 2)\n",
    "    v = v.view(B, C, self.n_heads, self.embed_size // self.n_heads).transpose(1, 2)\n",
    "    # by hand\n",
    "    correlation = q @ k.transpose(-2, -1)\n",
    "    correlation = correlation / math.sqrt(k.shape[-1])\n",
    "    correlation = F.softmax(correlation, dim=-1)\n",
    "    correlation = F.dropout(correlation, p=self.dropout_rate, training=self.training)\n",
    "    y = correlation @ v\n",
    "    # combine heads\n",
    "    y = y.transpose(1, 2).contiguous().view(B, C, self.embed_size)\n",
    "    y = self.dropout(y)\n",
    "    return y, correlation\n",
    "\n",
    "class Block(nn.Module):\n",
    "  def __init__(self, n_heads, embed_size, hidden_size, dropout, n_block):\n",
    "    super().__init__()\n",
    "    self.block = n_block\n",
    "    self.attention = MultiheadAttention(n_heads, embed_size, dropout=dropout, n_block=n_block)\n",
    "    self.ff = nn.Sequential(\n",
    "      nn.LayerNorm(embed_size), # pre-layernorm\n",
    "      nn.Linear(embed_size, hidden_size),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(hidden_size, embed_size),\n",
    "      nn.Dropout(dropout)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x + self.attention(x)\n",
    "    x = x + self.ff(x)\n",
    "    return x\n",
    "  \n",
    "  def forward_visualize(self, x):\n",
    "    x1, cor = self.attention.forward_visualize(x)\n",
    "    x = x + x1\n",
    "    x = x + self.ff(x)\n",
    "    return x, cor\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: explain what is the size and functionality of tile_embedding layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self, n_heads=N_HEADS, n_blocks=N_BLOCKS, embed_size=EMBED_SIZE, hidden_size=HIDDEN_SIZE, vocab_size=VOCAB_SIZE, context_size=CONTEXT_SIZE, dropout=DROPOUT, tile_size=TILE_SIZE):\n",
    "    super().__init__()\n",
    "    self.context_size = context_size\n",
    "    self.tile_size = tile_size\n",
    "    # net\n",
    "    self.positional_embedding = get_sinusoidal_positional_encoding()\n",
    "    self.tile_embedding = nn.Linear(TILE_SIZE * TILE_SIZE * CHANNEL, embed_size)\n",
    "    self.cls_token = nn.Parameter(torch.randn(1, 1, embed_size))\n",
    "    self.blocks = nn.Sequential(*[Block(n_heads, embed_size, hidden_size, dropout, i) for i in range(n_blocks)])\n",
    "    self.head = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # (batch_size, channel, height, width)\n",
    "    # split into tiles/patches/chunks\n",
    "    x = einops.rearrange(x, 'b c (h t1) (w t2) -> b (h w) (c t1 t2)', t1=self.tile_size, t2=self.tile_size)\n",
    "    # embed\n",
    "    x = self.tile_embedding(x)\n",
    "    # add cls token\n",
    "    cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "    x = torch.cat((cls_token, x), dim=1)\n",
    "    # positional encoding\n",
    "    x = x + self.positional_embedding\n",
    "    # attention and ff\n",
    "    x = self.blocks(x)\n",
    "    # head\n",
    "    x = self.head(x)\n",
    "    # only look at first token:\n",
    "    x = x[:, 0, :]\n",
    "    return x\n",
    "  \n",
    "  def forward_visualize(self, x):\n",
    "    # (batch_size, channel, height, width)\n",
    "    # split into tiles/patches/chunks\n",
    "    x = einops.rearrange(x, 'b c (h t1) (w t2) -> b (h w) (c t1 t2)', t1=self.tile_size, t2=self.tile_size)\n",
    "    # embed\n",
    "    x = self.tile_embedding(x)\n",
    "    # add cls token\n",
    "    cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "    x = torch.cat((cls_token, x), dim=1)\n",
    "    # positional encoding\n",
    "    x = x + self.positional_embedding\n",
    "    all_corrs = []\n",
    "    for block in self.blocks:\n",
    "      x, corr = block.forward_visualize(x)\n",
    "      all_corrs.append(corr.detach())\n",
    "    \n",
    "    x = self.head(x)\n",
    "    # only look at first token:\n",
    "    x = x[:, 0, :]\n",
    "    return x, all_corrs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Draw outline of this ViT, including the exact number of blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(n_heads=2, n_blocks=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resume training if you want\n",
    "# model.load_state_dict(torch.load('weights/vit_mnist_epoch20.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you don't have a GPU, borrow one checkpoint file from your classmate, and skip the training steps\n",
    "### If you have a 4060 or better GPU, try running for 50 steps or more, and check the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 0.24248148500919342\n",
      "    2 0.3604864776134491\n",
      "    4 0.18556368350982666\n",
      "    6 0.33382007479667664\n",
      "    8 0.5422306656837463\n",
      "   10 0.16104143857955933\n",
      "   12 0.19972656667232513\n",
      "   14 0.3403984308242798\n",
      "   16 0.2939150929450989\n",
      "   18 0.30659735202789307\n",
      "   20 0.39700207114219666\n",
      "   22 0.30767878890037537\n",
      "   24 0.17942719161510468\n",
      "   26 0.18721641600131989\n",
      "   28 0.22907942533493042\n",
      "   30 0.18222172558307648\n"
     ]
    }
   ],
   "source": [
    "# Training steps\n",
    "\n",
    "epochs = 31   # feel free to change to 100 \n",
    "lr = LEARNING_RATE\n",
    "model.train()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "log_every = 2\n",
    "save_every = 10\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  for X, Y in data:\n",
    "    X, Y = X.to(device), Y.to(device)\n",
    "    out = model(X)\n",
    "    loss = F.cross_entropy(out, Y)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "  if epoch % log_every == 0:\n",
    "    # print loss\n",
    "    print(f'{epoch:5} {loss.item()}')\n",
    "  \n",
    "  if epoch>5 and epoch % save_every == 0:\n",
    "    # save model\n",
    "    torch.save(model.state_dict(), 'weights/vit_mnist_epoch%02d.pt' % (epoch,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Wait for training finished, or you can interrupt, and load a saved model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('weights/vit_mnist_epoch30.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test : 0.8091999888420105\n",
      "accuracy on train: 0.8013833165168762\n"
     ]
    }
   ],
   "source": [
    "# test set accuracy\n",
    "_ = accuracy(model)\n",
    "# train set accuracy\n",
    "_ = accuracy(model, label='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def failing(model, ds=data_test):\n",
    "    model.eval()\n",
    "    misslabeled_imgs = []\n",
    "    losses = []\n",
    "    wrong_labels = []\n",
    "    real_labels = []\n",
    "    for x, y in ds:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        out = logits.argmax(-1)\n",
    "        losses.append(-logits.softmax(1)[torch.arange(logits.shape[0]), y].log()[out != y])\n",
    "        misslabeled_imgs.append(x[out != y])\n",
    "        wrong_labels.append(out[out != y])\n",
    "        real_labels.append(y[out != y])\n",
    "    return torch.cat(misslabeled_imgs), torch.cat(wrong_labels), torch.cat(real_labels), torch.cat(losses)\n",
    "\n",
    "def sorted_failing(model, ds=data_test, descending=True):\n",
    "    imgs, wrong_labels, real_labels, losses = failing(model, ds)\n",
    "    order = losses.argsort(descending=descending)\n",
    "    return imgs[order], wrong_labels[order], real_labels[order], losses[order]\n",
    "\n",
    "def plot_failing(imgs, wrong_labels, real_labels, n_col=7, figsize=(12, 4)):\n",
    "    fig, ax = plt.subplots(2, n_col, figsize=figsize)\n",
    "    \n",
    "    for i in range(imgs.shape[0]):\n",
    "        idx = (i // n_col, i % n_col)\n",
    "        img = imgs[i].cpu().permute(1, 2, 0)\n",
    "        ax[idx].imshow(img)\n",
    "        ax[idx].set_title(f'p={wrong_labels[i].item()} / {real_labels[i].item()} loss={losses[i].item():.2f}')\n",
    "        ax[idx].set_xticks([])\n",
    "        ax[idx].set_yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "misslabeled_imgs, wrong_labels, real_labels, losses = sorted_failing(model)\n",
    "plot_failing(misslabeled_imgs[:14], wrong_labels[:14], real_labels[:14])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot confusion matrix\n",
    "### TODO: We want to know which classes are misclassified the most.\n",
    "### TODO: Provide top-5 cases, that True class A is misclassified as B.  Print A->B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def label_all(model, ds=data_test):\n",
    "    model.eval()\n",
    "    predicted_labels = []\n",
    "    real_labels = []\n",
    "    for x, y in ds:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x).argmax(-1)\n",
    "        predicted_labels.append(out)\n",
    "        real_labels.append(y)\n",
    "    return torch.cat(predicted_labels), torch.cat(real_labels)\n",
    "\n",
    "def to_confusion_matrix(predicted_labels, real_labels):\n",
    "    confusion_matrix = torch.zeros((10, 10), dtype=torch.int32)\n",
    "    for p, r in zip(predicted_labels, real_labels):\n",
    "        confusion_matrix[r, p] += 1\n",
    "    return confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(confusion_matrix, figsize=(5, 4)):\n",
    "    num_classes = confusion_matrix.shape[0]\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(confusion_matrix, interpolation='nearest', cmap='Blues', norm=colors.LogNorm())\n",
    "    for i in range(num_classes):\n",
    "        for j in range(num_classes):\n",
    "            plt.text(j, i, int(confusion_matrix[i, j]),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if confusion_matrix[i, j] > confusion_matrix.max() / 2. else \"black\")\n",
    "    plt.title('confusion matrix')\n",
    "    plt.xlabel('predicted')\n",
    "    plt.ylabel('real')\n",
    "    plt.xticks(np.arange(num_classes))\n",
    "    plt.yticks(np.arange(num_classes))\n",
    "    plt.show()\n",
    "\n",
    "predicted_labels, real_labels = label_all(model)\n",
    "confusion_matrix = to_confusion_matrix(predicted_labels, real_labels)\n",
    "plot_confusion_matrix(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize attention\n",
    "### TODO: what do you find the activation map?  \n",
    "### Display one digit as example and explain which token i is mostly associated with token j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "@torch.no_grad()\n",
    "def visualize_attention(model, img):\n",
    "    model.eval()\n",
    "    plot(img)\n",
    "    tiles = einops.rearrange(img, 'c (h t1) (w t2) -> (h w) c t1 t2', t1=TILE_SIZE, t2=TILE_SIZE)\n",
    "    plot(tiles, n_col=4, block_size=0.75)\n",
    "    img = img.to(device).unsqueeze(0)\n",
    "    out, activations = model.forward_visualize(img)\n",
    "    out = out.argmax(-1).item()\n",
    "\n",
    "    plt.gca().axes.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    for block, acts in enumerate(activations):\n",
    "        # 17x17 attention magnitude\n",
    "        #print(acts.size())\n",
    "        acts = acts.squeeze()\n",
    "        acts = acts[1:,1:].cpu()\n",
    "        plt.imshow(acts)\n",
    "        \n",
    "visualize_attention(model, next(iter(data))[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize positional embeddings\n",
    "#### note that emb is (1 + patch_number) * embed_size\n",
    "#### we discard the first [cls], then visualize patch_number*embed_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def visualize_positional_embeddings(emb):\n",
    "    print(emb.size())\n",
    "    # strip cls token\n",
    "    emb = emb[1:]\n",
    "    emb = einops.rearrange(emb, 'n (k1 k2) -> n 1 k1 k2', k1=4)\n",
    "    plot(emb, n_col=4, block_size=1)\n",
    "\n",
    "visualize_positional_embeddings(model.positional_embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "videollava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
